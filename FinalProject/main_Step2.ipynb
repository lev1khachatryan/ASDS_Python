{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#for timestamp column\n",
    "from datetime import date\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs): pass\n",
    "warnings.warn = ignore_warn\n",
    "\n",
    "# machine learining\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "# Mathematics\n",
    "from math import log\n",
    "\n",
    "# Pandas options\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option('display.max_columns',500 )\n",
    "pd.set_option('display.max_rows',100 )\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "# pd.reset_option(\"display.max_rows\")\n",
    "\n",
    "# Save and Load Machine Learning Models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some auxiliary functions\n",
    "def detect_outliers(df,n,features):\n",
    "        \"\"\"\n",
    "        Takes a dataframe df of features and returns a list of the indices\n",
    "        corresponding to the observations containing more than n outliers according\n",
    "        to the Tukey method.\n",
    "        \"\"\"\n",
    "        outlier_indices = []\n",
    "        # iterate over features(columns)\n",
    "        for col in features:\n",
    "            # 1st quartile (25%)\n",
    "            Q1 = np.percentile(df[col], 25)\n",
    "            # 3rd quartile (75%)\n",
    "            Q3 = np.percentile(df[col],75)\n",
    "            # Interquartile range (IQR)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # outlier step\n",
    "            outlier_step = 1.5 * IQR\n",
    "\n",
    "            # Determine a list of indices of outliers for feature col\n",
    "            outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "\n",
    "            # append the found outlier indices for col to the list of outlier indices \n",
    "            outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "        # select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "        return multiple_outliers\n",
    "    \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "    \n",
    "def plot_feature_importances(df):\n",
    "        \"\"\"\n",
    "        Plot importances returned by a model. This can work with any measure of\n",
    "        feature importance provided that higher importance is better. \n",
    "\n",
    "        Args:\n",
    "            df (dataframe): feature importances. Must have the features in a column\n",
    "            called `features` and the importances in a column called `importance\n",
    "\n",
    "        Returns:\n",
    "            shows a plot of the 15 most importance features\n",
    "\n",
    "            df (dataframe): feature importances sorted by importance (highest to lowest) \n",
    "            with a column for normalized importance\n",
    "            \"\"\"\n",
    "\n",
    "        # Sort features according to importance\n",
    "        df = df.sort_values('importance', ascending = False).reset_index()\n",
    "\n",
    "        # Normalize the feature importances to add up to one\n",
    "        df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "        # Make a horizontal bar chart of feature importances\n",
    "        plt.figure(figsize = (10, 6))\n",
    "        ax = plt.subplot()\n",
    "\n",
    "        # Need to reverse the index to plot most important on top\n",
    "        ax.barh(list(reversed(list(df.index[:15]))), \n",
    "                df['importance_normalized'].head(15), \n",
    "                align = 'center', edgecolor = 'k')\n",
    "\n",
    "        # Set the yticks and labels\n",
    "        ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "        ax.set_yticklabels(df['feature'].head(15))\n",
    "\n",
    "        # Plot labeling\n",
    "        plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "        plt.show()\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def load_data(csv_name, load_from_disk = False):\n",
    "# #     if load_from_disk:\n",
    "# #         return pd.read_pickle(csv_name + '.pkl')\n",
    "    \n",
    "#     data = pd.read_csv(csv_name + '.csv')\n",
    "# #     data.to_pickle(csv_name + \".pkl\")\n",
    "# #     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_name, load_from_disk = False):\n",
    "    #if load_from_disk:\n",
    "    #    return pd.read_pickle('C:\\Users\\User\\Kaggle\\Final_Project\\' + csv_name + '.pkl')\n",
    "    \n",
    "    data = pd.read_csv('C:/Users/User/Kaggle/Final_Project/' + csv_name + '.csv')\n",
    "    #data.to_pickle(csv_name + \".pkl\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class descriptive_statistics:\n",
    "    def __init__(self, df, label):\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "        \n",
    "    def corrmat(self):\n",
    "        return self.df.corr()\n",
    "        \n",
    "    def most_correlated_features(self, print_heatmap = True ,tol=0.5):\n",
    "        corrmat = self.df.corr()\n",
    "        top_corr_features = corrmat.index[abs(corrmat[self.label]) > tol]\n",
    "        if print_heatmap:\n",
    "            plt.figure(figsize=(10,10))\n",
    "            sns.heatmap(self.df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "        return top_corr_features.values\n",
    "    \n",
    "#     def graph_between_most_correlated_features(self, tol=0.5):\n",
    "#         sns.set()\n",
    "#         corrmat = self.df.corr()\n",
    "#         top_corr_features = corrmat.index[abs(corrmat[self.label]) > tol]\n",
    "# #         corrmat.index[abs(corrmat[self.label]) > tol]\n",
    "#         cols = top_corr_features.values\n",
    "#         sns.pairplot(self[cols], size = 2.5)\n",
    "#         plt.show();\n",
    "    \n",
    "    def mean(self, feature):\n",
    "        return np.mean(self.df[feature])\n",
    "    \n",
    "    def median(self, feature):\n",
    "        return np.median(self.df[feature])\n",
    "#     def mode(self, feature):\n",
    "#         return np.mod(self.df[feature])\n",
    "    \n",
    "    def standard_deviation(self, feature):\n",
    "        return np.std(self.df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data, is_train_dataset = False):\n",
    "#     print('---------------------------------------')\n",
    "#     print('Starting preprocessing')\n",
    "#     print('---------------------------------------')\n",
    "#     print('checking the volume of missing values')\n",
    "#     print('---------------------------------------')\n",
    "#     missing_values_table(data)\n",
    "#     print('---------------------------------------')\n",
    "#     data.drop(\"Id\", axis = 1, inplace = True)\n",
    "    if is_train_dataset:\n",
    "#         print('Delete outliers for training dataset')\n",
    "#         print('---------------------------------------')\n",
    "        ddxk = descriptive_statistics(df=data,label='SalePrice' )\n",
    "        Outliers_to_drop = detect_outliers(data,2, ddxk.most_correlated_features(print_heatmap = False))\n",
    "        data = data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n",
    "    \n",
    "    data[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\n",
    "    data[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\n",
    "    data[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\n",
    "    data[\"Fence\"] = data[\"Fence\"].fillna(\"None\")\n",
    "    data[\"FireplaceQu\"] = data[\"FireplaceQu\"].fillna(\"None\")\n",
    "    data[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "        lambda x: x.fillna(x.median()))\n",
    "    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "        data[col] = data[col].fillna('None')\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "        data[col] = data[col].fillna(0)\n",
    "    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "        data[col] = data[col].fillna(0)\n",
    "    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "        data[col] = data[col].fillna('None')\n",
    "    data[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\n",
    "    data[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\n",
    "    data['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n",
    "    data = data.drop(['Utilities'], axis=1)\n",
    "    data[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\n",
    "    data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\n",
    "    data['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n",
    "    data['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\n",
    "    data['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n",
    "    data['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\n",
    "    data['MSSubClass'] = data['MSSubClass'].fillna(\"None\")\n",
    "    \n",
    "    #MSSubClass=The building class\n",
    "    data['MSSubClass'] = data['MSSubClass'].apply(str)\n",
    "\n",
    "    #Changing OverallCond into a categorical variable\n",
    "    data['OverallCond'] = data['OverallCond'].astype(str)\n",
    "\n",
    "\n",
    "    #Year and month sold are transformed into categorical features.\n",
    "    data['YrSold'] = data['YrSold'].astype(str)\n",
    "    data['MoSold'] = data['MoSold'].astype(str)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "            'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "            'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "            'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "            'YrSold', 'MoSold')\n",
    "    # process columns, apply LabelEncoder to categorical features\n",
    "    for c in cols:\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(data[c].values)) \n",
    "        data[c] = lbl.transform(list(data[c].values))\n",
    "    \n",
    "    # Adding total sqfootage feature \n",
    "    data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n",
    "    \n",
    "    data = pd.get_dummies(data)\n",
    "\n",
    "    # shape        \n",
    "#     print('Shape data: {}'.format(data.shape))\n",
    "#     print('---------------------------------------')\n",
    "#     print('The preprocessing stage is already done')\n",
    "#     print('---------------------------------------')\n",
    "#     print('checking the volume of missing values again')\n",
    "#     print('---------------------------------------')\n",
    "#     missing_values_table(data)    \n",
    "#     print('---------------------------------------')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for training data\n",
    "def label_distribution(data, label):\n",
    "    sns.distplot(data[label] , fit=norm);\n",
    "\n",
    "    # Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(data[label])\n",
    "    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "    #Now plot the distribution\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "                loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(label + ' distribution')\n",
    "\n",
    "    #Get also the QQ-plot\n",
    "    fig = plt.figure()\n",
    "    res = stats.probplot(data[label], plot=plt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformation(data, label, show_graph = False):\n",
    "    #We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "    data[label] = np.log1p(data[label])\n",
    "    if show_graph:\n",
    "        #Check the new distribution \n",
    "        sns.distplot(data[label] , fit=norm);\n",
    "\n",
    "        # Get the fitted parameters used by the function\n",
    "        (mu, sigma) = norm.fit(data[label])\n",
    "        print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "        #Now plot the distribution\n",
    "        plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "                    loc='best')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(label + ' distribution')\n",
    "\n",
    "        #Get also the QQ-plot\n",
    "        fig = plt.figure()\n",
    "        res = stats.probplot(data[label], plot=plt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "def rmsle_cv(model, X, Y):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X.values, Y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args, load_from_disk=False):\n",
    "#     train_data = load_data(csv_name='train', load_from_disk=False)\n",
    "#     print('********** Preprocessing Phase **********')\n",
    "#     train_data = data_preprocessing(data=train_data,is_train_dataset=True)\n",
    "#     print('********** Label Distribution **********')\n",
    "#     label_distribution(train_data, label='SalePrice')\n",
    "#     print('********** Log Transformation **********')\n",
    "#     log_transformation(data=train_data, label='SalePrice')\n",
    "    \n",
    "#     X = train_data.drop(columns=[\"SalePrice\"])\n",
    "#     Y = train_data[\"SalePrice\"]\n",
    "    \n",
    "#     test_size = 0.33\n",
    "#     seed = 7\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "#     # Fit the model on 33%\n",
    "# #     ddxk = X_test.head(1)\n",
    "# #     ddxk_value = Y_test.head(1)\n",
    "    \n",
    "#     lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "#     ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "#     KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "#     GBoost = GradientBoostingRegressor()\n",
    "#     model_xgb = xgb.XGBRegressor()\n",
    "#     model_lgb = lgb.LGBMRegressor(objective='regression')\n",
    "    \n",
    "#     print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(rmsle_cv(lasso, X_train, Y_train).mean(), rmsle_cv(lasso, X_train, Y_train).std()))\n",
    "#     print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(rmsle_cv(ENet, X_train, Y_train).mean(), rmsle_cv( ENet , X_train, Y_train).std()))\n",
    "#     print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(rmsle_cv(KRR, X_train, Y_train).mean(), rmsle_cv( KRR , X_train, Y_train).std()))\n",
    "#     print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(rmsle_cv(GBoost, X_train, Y_train).mean(), rmsle_cv( GBoost , X_train, Y_train).std()))\n",
    "#     print(\"XGboost score: {:.4f} ({:.4f})\\n\".format(rmsle_cv(model_xgb, X_train, Y_train).mean(), rmsle_cv( model_xgb , X_train, Y_train).std()))\n",
    "#     print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(rmsle_cv(model_lgb, X_train, Y_train).mean(), rmsle_cv( model_lgb , X_train, Y_train).std()))\n",
    "\n",
    "#     averaged_models = AveragingModels(models = (lasso , ENet , GBoost, model_xgb, model_lgb))\n",
    "#     print(\"Averaged base models score: {:.4f} ({:.4f})\\n\".format(rmsle_cv(averaged_models, X_train, Y_train).mean(), rmsle_cv(averaged_models, X_train, Y_train).std()))\n",
    "    \n",
    "    \n",
    "# #     test_data = load_data(csv_name='test', load_from_disk=False)\n",
    "# #     test_data = data_preprocessing(data=test_data,is_train_dataset=False)\n",
    "# #     test_data.head()\n",
    "# #     missing_cols = set(X.columns ) - set(test_data.columns )\n",
    "# #     # Add a missing column in test set with default value equal to 0\n",
    "# #     for c in missing_cols:\n",
    "# #         test_data[c] = 0\n",
    "# #     # Ensure the order of column in the test set is in the same order than in train set\n",
    "# #     test_data = X[X.columns]\n",
    "# # #     y_pred = boost.predict(test_X)[0]\n",
    "# # #     print(\"predicted duration is %f days\" % y_pred)\n",
    "# # #     print(\"actual duration is %f days\" % test_Y)\n",
    "    \n",
    "# #     print(\"--------------------------------\")\n",
    "# #     print(\"--------------------------------\")\n",
    "# #     print(\"--------------------------------\")\n",
    "# #     print(cross_val_score(averaged_models, X_train, Y_train, cv=5, verbose=True))\n",
    "    \n",
    "    \n",
    "#     averaged_models.fit(X_train, Y_train)\n",
    "#     averaged_train_pred = averaged_models.predict(X_train)\n",
    "#     y_pred = np.expm1(averaged_models.predict(X_test))#[0]\n",
    "#     print(\"\\nMy final model is averaged model\\n\")\n",
    "#     print(\"RMSE : {:.4f}\\n\".format(rmsle(Y_train, averaged_train_pred)))\n",
    "    \n",
    "#     X_test.loc[:,'Outcome'] = y_pred\n",
    "#     print(\"training has been completed succesfully !!!!\")\n",
    "#     print(\"--------------------------------------------\")\n",
    "#     X_test[\"Actual_Values\"] = np.expm1(Y_test)\n",
    "# #     print(\"predicted sale price value is %f \" % y_pred)\n",
    "#     print(\"Prediction\")\n",
    "#     print(X_test[['Actual_Values','Outcome']])\n",
    "    \n",
    "# #     print(\"actual sale price is %f \" % np.expm1(ddxk_value))\n",
    "    \n",
    "    \n",
    "# #     filename = 'finalized_model.sav'\n",
    "# #     pickle.dump(averaged_models, open(filename, 'wb'))\n",
    "    \n",
    "    \n",
    "# #     # some time later...\n",
    "\n",
    "# #     # load the model from disk\n",
    "# #     loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# #     result = loaded_model.score(X_test, Y_test)\n",
    "# #     print(\"Results on testing data set !!!\\n\")\n",
    "# #     print(\"Score on test data set : {:.4f}\".format(result))\n",
    "\n",
    "# main(args=None, load_from_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "    KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "    GBoost = GradientBoostingRegressor()\n",
    "    model_xgb = xgb.XGBRegressor()\n",
    "    model_lgb = lgb.LGBMRegressor(objective='regression')\n",
    "    averaged_models = AveragingModels(models = (lasso , ENet , GBoost, model_xgb, model_lgb))\n",
    "    train_data = load_data(csv_name='train', load_from_disk=False)\n",
    "    train_data = data_preprocessing(data=train_data,is_train_dataset=True)\n",
    "#     label_distribution(train_data, label='SalePrice')\n",
    "    log_transformation(data=train_data, label='SalePrice', show_graph=False)\n",
    "    X = train_data.drop(columns=[\"SalePrice\"])\n",
    "    Y = train_data[\"SalePrice\"]\n",
    "    averaged_models.fit(X, Y)\n",
    "    np.save('col.npy', X.columns)\n",
    "    print(\"training has been completed succesfully !!!!\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    filename = 'finalized_model.sav'\n",
    "    pickle.dump(averaged_models, open(filename, 'wb'))\n",
    "    return averaged_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here supposed that test_X is a dataframe with 1 row, for which we must make a prediction\n",
    "def prediction(model, test_X):\n",
    "    train_col = np.load('col.npy')\n",
    "#     test_X = data_preprocessing(data=test_X, is_train_dataset=False)\n",
    "\n",
    "    missing_cols = set(train_col) - set(test_X.columns )\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        test_X[c] = 0\n",
    "    # Ensure the order of column in the test set is in the same order than in train set\n",
    "    test_X = test_X[train_col]\n",
    "    y_pred = model.predict(test_X)[0]    \n",
    "    return np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data = load_data(csv_name='train', load_from_disk=False)\n",
    "# ddd = descriptive_statistics(df = train_data, label='SalePrice')\n",
    "# mcf = ddd.most_correlated_features(print_heatmap=False)\n",
    "# print(mcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    'OverallQual' : 5 ,\n",
    "    'YearBuilt' : 1961,\n",
    "    'YearRemodAdd' : 1961,\n",
    "    'TotalBsmtSF' : 882.000 ,\n",
    "    '1stFlrSF' : 896,\n",
    "    'GrLivArea' : 896,\n",
    "    'FullBath' : 1,\n",
    "    'TotRmsAbvGrd' : 5,\n",
    "    'GarageCars' : 1.000,\n",
    "    'GarageArea' : 730.000\n",
    "}\n",
    "\n",
    "# /api/foo/?OverallQual=5&YearBuilt=1961&YearRemodAdd=1961&TotalBsmtSF=882.000&1stFlrSF=896&GrLivArea=896&FullBath=1&TotRmsAbvGrd=5&GarageCars=1.000&GarageArea=730.000\n",
    "# http://localhost:5000/api/foo/?OverallQual=5&YearBuilt=1961&YearRemodAdd=1961&TotalBsmtSF=882&1stFlrSF=896&GrLivArea=896&FullBath=1&TotRmsAbvGrd=5&GarageCars=1&GarageArea=730\n",
    "# df = pd.DataFrame.from_dict(dict,orient='columns')\n",
    "df = pd.DataFrame(dict, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = load_data(csv_name='test', load_from_disk=False)\n",
    "# # test_data.head(1)\n",
    "# obs = test_data[['OverallQual', 'YearBuilt', 'YearRemodAdd' ,'TotalBsmtSF' ,'1stFlrSF', 'GrLivArea' ,'FullBath' ,'TotRmsAbvGrd' ,'GarageCars', 'GarageArea']].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training has been completed succesfully !!!!\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65760.92965667119"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = init()\n",
    "prediction(my_model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_col = np.load('col.npy')\n",
    "# averaged_models = pickle.load(open('finalized_model.sav', 'rb'))\n",
    "# prediction(averaged_models, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args(*argument_array):\n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument('--load_model', '-l'\n",
    "#                         ,help='load model from disk')\n",
    "#     parser.add_argument('--load_from_disk', '-ld'\n",
    "#                         , help='load data from disk')\n",
    "#     args = parser.parse_args(*argument_array)\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "#     if args.load_model:\n",
    "#         averaged_models = pickle.load(open('finalized_model.sav', 'rb'))\n",
    "#     else:\n",
    "#         averaged_models = init()\n",
    "#         pickle.dump(averaged_models, open('finalized_model.sav', 'wb'))\n",
    "#     while True:\n",
    "#         predict(averaged_models,args)\n",
    "# if __name__ == '__main__':\n",
    "#     args = parse_args()\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.externals import joblib\n",
    "# # boost = joblib.load('finalized_model.sav')\n",
    "# loaded_model = pickle.load(open('C:/Users/User/Kaggle/Final_Project/finalized_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_col = np.load('col.npy')\n",
    "# print(train_col)\n",
    "train_col = ['Id' 'MSSubClass' 'LotFrontage' 'LotArea' 'Street' 'Alley' 'LotShape'\n",
    " 'LandSlope' 'OverallQual' 'OverallCond' 'YearBuilt' 'YearRemodAdd'\n",
    " 'MasVnrArea' 'ExterQual' 'ExterCond' 'BsmtQual' 'BsmtCond' 'BsmtExposure'\n",
    " 'BsmtFinType1' 'BsmtFinSF1' 'BsmtFinType2' 'BsmtFinSF2' 'BsmtUnfSF'\n",
    " 'TotalBsmtSF' 'HeatingQC' 'CentralAir' '1stFlrSF' '2ndFlrSF'\n",
    " 'LowQualFinSF' 'GrLivArea' 'BsmtFullBath' 'BsmtHalfBath' 'FullBath'\n",
    " 'HalfBath' 'BedroomAbvGr' 'KitchenAbvGr' 'KitchenQual' 'TotRmsAbvGrd'\n",
    " 'Functional' 'Fireplaces' 'FireplaceQu' 'GarageYrBlt' 'GarageFinish'\n",
    " 'GarageCars' 'GarageArea' 'GarageQual' 'GarageCond' 'PavedDrive'\n",
    " 'WoodDeckSF' 'OpenPorchSF' 'EnclosedPorch' '3SsnPorch' 'ScreenPorch'\n",
    " 'PoolArea' 'PoolQC' 'Fence' 'MiscVal' 'MoSold' 'YrSold' 'TotalSF'\n",
    " 'MSZoning_C (all)' 'MSZoning_FV' 'MSZoning_RH' 'MSZoning_RL'\n",
    " 'MSZoning_RM' 'LandContour_Bnk' 'LandContour_HLS' 'LandContour_Low'\n",
    " 'LandContour_Lvl' 'LotConfig_Corner' 'LotConfig_CulDSac' 'LotConfig_FR2'\n",
    " 'LotConfig_FR3' 'LotConfig_Inside' 'Neighborhood_Blmngtn'\n",
    " 'Neighborhood_Blueste' 'Neighborhood_BrDale' 'Neighborhood_BrkSide'\n",
    " 'Neighborhood_ClearCr' 'Neighborhood_CollgCr' 'Neighborhood_Crawfor'\n",
    " 'Neighborhood_Edwards' 'Neighborhood_Gilbert' 'Neighborhood_IDOTRR'\n",
    " 'Neighborhood_MeadowV' 'Neighborhood_Mitchel' 'Neighborhood_NAmes'\n",
    " 'Neighborhood_NPkVill' 'Neighborhood_NWAmes' 'Neighborhood_NoRidge'\n",
    " 'Neighborhood_NridgHt' 'Neighborhood_OldTown' 'Neighborhood_SWISU'\n",
    " 'Neighborhood_Sawyer' 'Neighborhood_SawyerW' 'Neighborhood_Somerst'\n",
    " 'Neighborhood_StoneBr' 'Neighborhood_Timber' 'Neighborhood_Veenker'\n",
    " 'Condition1_Artery' 'Condition1_Feedr' 'Condition1_Norm'\n",
    " 'Condition1_PosA' 'Condition1_PosN' 'Condition1_RRAe' 'Condition1_RRAn'\n",
    " 'Condition1_RRNe' 'Condition1_RRNn' 'Condition2_Artery'\n",
    " 'Condition2_Feedr' 'Condition2_Norm' 'Condition2_PosA' 'Condition2_RRAe'\n",
    " 'Condition2_RRAn' 'Condition2_RRNn' 'BldgType_1Fam' 'BldgType_2fmCon'\n",
    " 'BldgType_Duplex' 'BldgType_Twnhs' 'BldgType_TwnhsE' 'HouseStyle_1.5Fin'\n",
    " 'HouseStyle_1.5Unf' 'HouseStyle_1Story' 'HouseStyle_2.5Fin'\n",
    " 'HouseStyle_2.5Unf' 'HouseStyle_2Story' 'HouseStyle_SFoyer'\n",
    " 'HouseStyle_SLvl' 'RoofStyle_Flat' 'RoofStyle_Gable' 'RoofStyle_Gambrel'\n",
    " 'RoofStyle_Hip' 'RoofStyle_Mansard' 'RoofStyle_Shed' 'RoofMatl_CompShg'\n",
    " 'RoofMatl_Membran' 'RoofMatl_Metal' 'RoofMatl_Roll' 'RoofMatl_Tar&Grv'\n",
    " 'RoofMatl_WdShake' 'RoofMatl_WdShngl' 'Exterior1st_AsbShng'\n",
    " 'Exterior1st_AsphShn' 'Exterior1st_BrkComm' 'Exterior1st_BrkFace'\n",
    " 'Exterior1st_CBlock' 'Exterior1st_CemntBd' 'Exterior1st_HdBoard'\n",
    " 'Exterior1st_ImStucc' 'Exterior1st_MetalSd' 'Exterior1st_Plywood'\n",
    " 'Exterior1st_Stone' 'Exterior1st_Stucco' 'Exterior1st_VinylSd'\n",
    " 'Exterior1st_Wd Sdng' 'Exterior1st_WdShing' 'Exterior2nd_AsbShng'\n",
    " 'Exterior2nd_AsphShn' 'Exterior2nd_Brk Cmn' 'Exterior2nd_BrkFace'\n",
    " 'Exterior2nd_CBlock' 'Exterior2nd_CmentBd' 'Exterior2nd_HdBoard'\n",
    " 'Exterior2nd_ImStucc' 'Exterior2nd_MetalSd' 'Exterior2nd_Other'\n",
    " 'Exterior2nd_Plywood' 'Exterior2nd_Stone' 'Exterior2nd_Stucco'\n",
    " 'Exterior2nd_VinylSd' 'Exterior2nd_Wd Sdng' 'Exterior2nd_Wd Shng'\n",
    " 'MasVnrType_BrkCmn' 'MasVnrType_BrkFace' 'MasVnrType_None'\n",
    " 'MasVnrType_Stone' 'Foundation_BrkTil' 'Foundation_CBlock'\n",
    " 'Foundation_PConc' 'Foundation_Slab' 'Foundation_Stone' 'Foundation_Wood'\n",
    " 'Heating_Floor' 'Heating_GasA' 'Heating_GasW' 'Heating_Grav'\n",
    " 'Heating_OthW' 'Heating_Wall' 'Electrical_FuseA' 'Electrical_FuseF'\n",
    " 'Electrical_FuseP' 'Electrical_Mix' 'Electrical_SBrkr'\n",
    " 'GarageType_2Types' 'GarageType_Attchd' 'GarageType_Basment'\n",
    " 'GarageType_BuiltIn' 'GarageType_CarPort' 'GarageType_Detchd'\n",
    " 'GarageType_None' 'MiscFeature_Gar2' 'MiscFeature_None'\n",
    " 'MiscFeature_Othr' 'MiscFeature_Shed' 'MiscFeature_TenC' 'SaleType_COD'\n",
    " 'SaleType_CWD' 'SaleType_Con' 'SaleType_ConLD' 'SaleType_ConLI'\n",
    " 'SaleType_ConLw' 'SaleType_New' 'SaleType_Oth' 'SaleType_WD'\n",
    " 'SaleCondition_Abnorml' 'SaleCondition_AdjLand' 'SaleCondition_Alloca'\n",
    " 'SaleCondition_Family' 'SaleCondition_Normal' 'SaleCondition_Partial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some functions for Python Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(csv_name='train', load_from_disk=False)\n",
    "Stat = descriptive_statistics(df=df, label='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stat.most_correlated_features(print_heatmap=False, tol=0.7)\n",
    "# Stat.mean(feature='OverallQual')\n",
    "# Stat.median(feature='OverallQual')\n",
    "# df = pd.DataFrame(Stat.most_correlated_features(print_heatmap=False))\n",
    "# df\n",
    "a = pd.DataFrame(np.delete(Stat.most_correlated_features(print_heatmap=False), 10))\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2 = {\n",
    "    'OverallQual' : \"5\" ,\n",
    "    'YearBuilt' : \"1961\",\n",
    "    'YearRemodAdd' : \"1961\",\n",
    "    'TotalBsmtSF' : \"882.000\" ,\n",
    "    '1stFlrSF' : \"896\",\n",
    "    'GrLivArea' : \"896\",\n",
    "    'FullBath' : \"1\",\n",
    "    'TotRmsAbvGrd' : \"5\",\n",
    "    'GarageCars' : \"1.000\",\n",
    "    'GarageArea' : \"730.000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65760.92965667119"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newDict = dict(zip(dict2.keys(), [float(value) for value in dict2.values()]))\n",
    "newDict = {} # an empty dictionary\n",
    "for key, value in dict2.items(): # get the (key, value) tuples one at a time\n",
    "    newDict[key] = float(value)\n",
    "df2 = pd.DataFrame(newDict, index=[0])\n",
    "prediction(my_model, df2)\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
