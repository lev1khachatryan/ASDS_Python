{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">unittest â€” Unit testing framework</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***unittest*** unit testing framework was originally inspired by ***JUnit*** and has a similar flavor as major unit testing frameworks in other languages. It supports test automation, sharing of setup and shutdown code for tests, aggregation of tests into collections, and independence of the tests from the reporting framework.\n",
    "\n",
    "To achieve this, unittest supports some important concepts in an object-oriented way:\n",
    "\n",
    "* ***test fixture*** - A test fixture represents the preparation needed to perform one or more tests, and any associate cleanup actions. This may involve, for example, creating temporary or proxy databases, directories, or starting a server process.\n",
    "\n",
    "\n",
    "* ***test case*** - A test case is the individual unit of testing. It checks for a specific response to a particular set of inputs. unittest provides a base class, TestCase, which may be used to create new test cases.\n",
    "\n",
    "\n",
    "* ***test suite*** - A test suite is a collection of test cases, test suites, or both. It is used to aggregate tests that should be executed together.\n",
    "\n",
    "\n",
    "* ***test runner*** - A test runner is a component which orchestrates the execution of tests and provides the outcome to the user. The runner may use a graphical interface, a textual interface, or return a special value to indicate the results of executing the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example\n",
    "The unittest module provides a rich set of tools for constructing and running tests. This section demonstrates that a small subset of the tools suffice to meet the needs of most users.\n",
    "\n",
    "Here is a short script to test three string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # check that s.split fails when the separator is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A testcase is created by subclassing unittest.TestCase. The three individual tests are defined with methods whose names start with the letters ***test***. This naming convention informs the test runner about which methods represent tests.\n",
    "\n",
    "The crux of each test is a call to ***assertEqual()*** to check for an expected result; ***assertTrue()*** or ***assertFalse()*** to verify a condition; or ***assertRaises()**** to verify that a specific exception gets raised. These methods are used instead of the assert statement so the test runner can accumulate all test results and produce a report.\n",
    "\n",
    "The ***setUp()*** and ***tearDown()*** methods allow you to define instructions that will be executed before and after each test method. They are covered in more detail in the section Organizing test code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, iterations=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations    = iterations\n",
    "        \n",
    "        self._weights      = None\n",
    "        self._errors       = []\n",
    "        self._threshold    = 0.0\n",
    "\n",
    "    \n",
    "    def fit(self, training_vector, target_value):\n",
    "        \"\"\"Fit training data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_vector : {array-like}, shape = [samples, features]\n",
    "        target_value    : {array-life}, shape = [samples]\n",
    "        \"\"\"\n",
    "        numberOfSamples, numberOfFeatures = training_vector.shape\n",
    "        self._weights = np.zeros(numberOfFeatures)\n",
    "        \n",
    "        for _ in range(self.iterations):\n",
    "            errors = 0\n",
    "            for observation, target in zip(training_vector, target_value):\n",
    "                update          = self.learning_rate * (target - self.predict(observation))\n",
    "                self._weights  += update * observation\n",
    "                self._threshold = update\n",
    "                errors += int(update != 0.0)\n",
    "            self._errors.append(errors)\n",
    "            \n",
    "    def _net_input(self, training_vector):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(training_vector, self._weights)\n",
    "    \n",
    "    def predict(self, training_vector):\n",
    "        \"\"\"Return class label after unit step\n",
    "        Given an array of features, returns an array with \n",
    "        \"\"\"\n",
    "        #This is a confusing line. Explanation: \n",
    "        #np.where will first return indices of elements for which the net input is \n",
    "        #greater than the threshold. It will then mark them with 1 for match,\n",
    "        #or -1 for not a match.\n",
    "        # E.g  INPUT : training_vector = [10,2, -1, -2], threshold = 0\n",
    "        #      RETURN: np.array([1,1,-1,-1])\n",
    "        return np.where(self._net_input(training_vector)>= self._threshold, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fit (__main__.TestPreceptron) ... ERROR\n",
      "test_initialization (__main__.TestPreceptron) ... ok\n",
      "test_net_input (__main__.TestPreceptron) ... ERROR\n",
      "test_predict (__main__.TestPreceptron) ... ERROR\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_fit (__main__.TestPreceptron)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-0dacc77e21ca>\", line 58, in test_fit\n",
      "    training_set = pd.DataFrame.from_items(data.items(),\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_net_input (__main__.TestPreceptron)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-0dacc77e21ca>\", line 15, in test_net_input\n",
      "    training_vector = pd.DataFrame({'feature1':[1,1],\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_predict (__main__.TestPreceptron)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-0dacc77e21ca>\", line 37, in test_predict\n",
      "    training_set = pd.DataFrame.from_items(data.items(),\n",
      "NameError: name 'pd' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.004s\n",
      "\n",
      "FAILED (errors=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=3 failures=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "import unittest\n",
    "class TestPreceptron(unittest.TestCase):\n",
    "    \n",
    "    def test_initialization(self):\n",
    "        \n",
    "        a_perceptron = Perceptron(learning_rate=0.5, iterations=10)\n",
    "        self.assertIsNotNone(a_perceptron)\n",
    "        self.assertEqual    (a_perceptron.learning_rate, 0.5)\n",
    "        self.assertEqual    (a_perceptron.iterations   ,  10)\n",
    "    \n",
    "    def test_net_input(self):\n",
    "         \n",
    "        a_perceptron    = Perceptron()\n",
    "        training_vector = pd.DataFrame({'feature1':[1,1], \n",
    "                                        'feature2':[0,1]})\n",
    "        a_perceptron._weights = [0,1]\n",
    "        self.assertTrue(np.array_equal(\n",
    "                        a_perceptron._net_input(training_vector), \n",
    "                        [0,1]))\n",
    "            \n",
    "        a_perceptron._weights = [2,2]\n",
    "        self.assertTrue(np.array_equal(\n",
    "                        a_perceptron._net_input(training_vector), \n",
    "                        [2,4]))\n",
    "        \n",
    "    def test_predict(self):\n",
    "        \n",
    "        a_perceptron    = Perceptron()\n",
    "        \n",
    "        columns = ['Classification', 'FeatureA', 'FeatureB']\n",
    "        data    = {'obs1': [ 1,0,1],\n",
    "                   'obs2': [ 1,0,1],\n",
    "                   'obs3': [-1,1,0],\n",
    "                   'obs4': [-1,1,0]\n",
    "                   }\n",
    "        training_set = pd.DataFrame.from_items(data.items(),\n",
    "                                               orient='index',\n",
    "                                              columns=columns)\n",
    "        a_perceptron._weights    = [0,1]\n",
    "        a_perceptron._threshold  = 1.0 \n",
    "        #since all the weight is on Feature B the prediction must \n",
    "        #equal the original classification\n",
    "        self.assertTrue(np.array_equal(\n",
    "                         a_perceptron.predict(\n",
    "                         training_set[['FeatureA', 'FeatureB']]),\n",
    "                         training_set['Classification']))\n",
    "        \n",
    "    def test_fit(self):\n",
    "        \n",
    "        a_perceptron = Perceptron()\n",
    "        columns = ['Classification', 'FeatureA', 'FeatureB']\n",
    "        data    = {'obs1': [ 1,0,1],\n",
    "                   'obs2': [ 1,0,1],\n",
    "                   'obs3': [-1,1,0],\n",
    "                   'obs4': [-1,1,0]\n",
    "                   }\n",
    "        training_set = pd.DataFrame.from_items(data.items(),\n",
    "                                               orient='index',\n",
    "                                              columns=columns)\n",
    "        \n",
    "        a_perceptron.fit(training_set[['FeatureA', 'FeatureB']].values, \n",
    "                         training_set['Classification'].values)\n",
    "        \n",
    "        self.assertEqual(a_perceptron.predict([0,10]),[1])\n",
    "        self.assertEqual(a_perceptron.predict([10,0]),[-1])\n",
    "        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestPreceptron)\n",
    "unittest.TextTestRunner(verbosity=4).run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fit (__main__.TestPreceptron) ... C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "ok\n",
      "test_initialization (__main__.TestPreceptron) ... ok\n",
      "test_net_input (__main__.TestPreceptron) ... ok\n",
      "test_predict (__main__.TestPreceptron) ... C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.015s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestPreceptron)\n",
    "unittest.TextTestRunner(verbosity=4).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
