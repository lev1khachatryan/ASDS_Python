{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Layer,Lambda, Dropout\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import neural_network\n",
    "\n",
    "#hide wornings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "IDtest = test[\"PassengerId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Mr. Charles Alexander</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.00</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Miss. Mabel Helen</td>\n",
       "      <td>female</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.00</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Master. Thomas Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Constance Gladys</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>325</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Mr. George John Jr</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>342</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fortune, Miss. Alice Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>19950</td>\n",
       "      <td>263.00</td>\n",
       "      <td>C23 C25 C27</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>793</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Stella Anna</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Mr. Douglas Bullen</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>864</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Dorothy Edith \"Dolly\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                               Name     Sex  \\\n",
       "27            28         0       1     Fortune, Mr. Charles Alexander    male   \n",
       "88            89         1       1         Fortune, Miss. Mabel Helen  female   \n",
       "159          160         0       3         Sage, Master. Thomas Henry    male   \n",
       "180          181         0       3       Sage, Miss. Constance Gladys  female   \n",
       "201          202         0       3                Sage, Mr. Frederick    male   \n",
       "324          325         0       3           Sage, Mr. George John Jr    male   \n",
       "341          342         1       1     Fortune, Miss. Alice Elizabeth  female   \n",
       "792          793         0       3            Sage, Miss. Stella Anna  female   \n",
       "846          847         0       3           Sage, Mr. Douglas Bullen    male   \n",
       "863          864         0       3  Sage, Miss. Dorothy Edith \"Dolly\"  female   \n",
       "\n",
       "      Age  SibSp  Parch    Ticket    Fare        Cabin Embarked  \n",
       "27   19.0      3      2     19950  263.00  C23 C25 C27        S  \n",
       "88   23.0      3      2     19950  263.00  C23 C25 C27        S  \n",
       "159   NaN      8      2  CA. 2343   69.55          NaN        S  \n",
       "180   NaN      8      2  CA. 2343   69.55          NaN        S  \n",
       "201   NaN      8      2  CA. 2343   69.55          NaN        S  \n",
       "324   NaN      8      2  CA. 2343   69.55          NaN        S  \n",
       "341  24.0      3      2     19950  263.00  C23 C25 C27        S  \n",
       "792   NaN      8      2  CA. 2343   69.55          NaN        S  \n",
       "846   NaN      8      2  CA. 2343   69.55          NaN        S  \n",
       "863   NaN      8      2  CA. 2343   69.55          NaN        S  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[Outliers_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "dataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age             256\n",
       "Cabin          1007\n",
       "Embarked          2\n",
       "Fare              1\n",
       "Name              0\n",
       "Parch             0\n",
       "PassengerId       0\n",
       "Pclass            0\n",
       "Sex               0\n",
       "SibSp             0\n",
       "Survived        418\n",
       "Ticket            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty-n ev Nan - replace anem Nan-ov\n",
    "dataset = dataset.fillna(np.nan)\n",
    "\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\n",
    "# g = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\n",
    "# g.set_xlabel(\"Age\")\n",
    "# g.set_ylabel(\"Frequency\")\n",
    "# g = g.legend([\"Not Survived\",\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Fare missing values with the median value\n",
    "dataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.Embarked.dropna().mode()[0]  # 'S'\n",
    "dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Sex into categorical value 0 for male and 1 for female\n",
    "dataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "for i in index_NaN_age :\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        dataset['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Mr\n",
       "1     Mrs\n",
       "2    Miss\n",
       "3     Mrs\n",
       "4      Mr\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n",
    "dataset[\"Title\"] = pd.Series(dataset_title)\n",
    "dataset[\"Title\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "dataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n",
    "dataset[\"Title\"] = dataset[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = dataset)\n",
    "# g = g.set_ylabels(\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\n",
    "dataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n",
    "dataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
    "dataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to indicator values Title and Embarked \n",
    "dataset = pd.get_dummies(dataset, columns = [\"Title\"])\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        A5\n",
       "1        PC\n",
       "2    STONO2\n",
       "3         X\n",
       "4         X\n",
       "Name: Ticket, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n",
    "\n",
    "Ticket = []\n",
    "for i in list(dataset.Ticket):\n",
    "    if not i.isdigit() :\n",
    "        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n",
    "    else:\n",
    "        Ticket.append(\"X\")\n",
    "        \n",
    "dataset[\"Ticket\"] = Ticket\n",
    "dataset[\"Ticket\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")\n",
    "\n",
    "dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\n",
    "dataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\n",
    "\n",
    "dataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate train dataset and test dataset\n",
    "\n",
    "train = dataset[:train_len]\n",
    "test = dataset[train_len:]\n",
    "test.drop(labels=[\"Survived\"],axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate train features and label \n",
    "\n",
    "train[\"Survived\"] = train[\"Survived\"].astype(int)\n",
    "\n",
    "y = train[\"Survived\"]\n",
    "\n",
    "X = train.drop(labels = [\"Survived\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563, 66)\n",
      "(177, 66)\n",
      "(563,)\n",
      "(177,)\n",
      "(141, 66)\n",
      "(141,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainY.shape)\n",
    "print(testY.shape)\n",
    "print(valX.shape)\n",
    "print(valY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getModel(arr):\n",
    "#     model=Sequential()\n",
    "#     for i in range(len(arr)):\n",
    "#         if i!=0 and i!=len(arr)-1:\n",
    "#             if i==1:\n",
    "#                 model.add(Dense(arr[i],input_dim=arr[0],kernel_initializer='normal', activation='relu'))\n",
    "#             else:\n",
    "#                 model.add(Dense(arr[i],activation='relu'))\n",
    "#     model.add(Dense(arr[-1],kernel_initializer='normal',activation=\"sigmoid\"))\n",
    "#     model.compile(loss=\"binary_crossentropy\",optimizer='rmsprop',metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "ManualModel = Sequential()\n",
    "ManualModel.add(Dense(100, kernel_initializer='normal',activation='tanh' ))\n",
    "ManualModel.add(Dropout(0.2))\n",
    "ManualModel.add(Dense(50, kernel_initializer='normal',activation='tanh' ))\n",
    "ManualModel.add(Dropout(0.2))\n",
    "ManualModel.add(Dense(25, input_dim = 50, kernel_initializer='normal',activation='relu' ))\n",
    "ManualModel.add(Dropout(0.2))\n",
    "ManualModel.add(Dense(10, input_dim = 25, kernel_initializer='normal',activation='relu' ))\n",
    "ManualModel.add(Dropout(0.1))\n",
    "ManualModel.add(Dense(1, kernel_initializer='normal'))\n",
    "ManualModel.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstModel=getModel([66,50,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 563 samples, validate on 141 samples\n",
      "Epoch 1/200\n",
      "563/563 [==============================] - 1s 2ms/step - loss: 1.4984 - acc: 0.6252 - val_loss: 1.1132 - val_acc: 0.5887\n",
      "Epoch 2/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.9063 - acc: 0.6252 - val_loss: 0.8112 - val_acc: 0.5887\n",
      "Epoch 3/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.7209 - acc: 0.6252 - val_loss: 0.7119 - val_acc: 0.5887\n",
      "Epoch 4/200\n",
      "563/563 [==============================] - 0s 42us/step - loss: 0.6834 - acc: 0.5897 - val_loss: 0.6998 - val_acc: 0.5887\n",
      "Epoch 5/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.6840 - acc: 0.5915 - val_loss: 0.6965 - val_acc: 0.5887\n",
      "Epoch 6/200\n",
      "563/563 [==============================] - 0s 45us/step - loss: 0.6778 - acc: 0.6270 - val_loss: 0.6979 - val_acc: 0.5887\n",
      "Epoch 7/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.6694 - acc: 0.6163 - val_loss: 0.6951 - val_acc: 0.5887\n",
      "Epoch 8/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.6939 - acc: 0.6252 - val_loss: 0.6945 - val_acc: 0.5887\n",
      "Epoch 9/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.6705 - acc: 0.6163 - val_loss: 0.6848 - val_acc: 0.5887\n",
      "Epoch 10/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.6713 - acc: 0.6412 - val_loss: 0.6791 - val_acc: 0.5887\n",
      "Epoch 11/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.6676 - acc: 0.6252 - val_loss: 0.6739 - val_acc: 0.5887\n",
      "Epoch 12/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.6602 - acc: 0.6288 - val_loss: 0.6511 - val_acc: 0.5887\n",
      "Epoch 13/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.6223 - acc: 0.6412 - val_loss: 0.6050 - val_acc: 0.5887\n",
      "Epoch 14/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.5845 - acc: 0.6892 - val_loss: 0.5365 - val_acc: 0.7234\n",
      "Epoch 15/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.5251 - acc: 0.7638 - val_loss: 0.5366 - val_acc: 0.7021\n",
      "Epoch 16/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.5055 - acc: 0.8028 - val_loss: 0.4750 - val_acc: 0.7660\n",
      "Epoch 17/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4933 - acc: 0.8206 - val_loss: 0.4570 - val_acc: 0.7943\n",
      "Epoch 18/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.4625 - acc: 0.8135 - val_loss: 0.4649 - val_acc: 0.7589\n",
      "Epoch 19/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.5089 - acc: 0.7851 - val_loss: 0.8946 - val_acc: 0.6454\n",
      "Epoch 20/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.7172 - acc: 0.6767 - val_loss: 0.6268 - val_acc: 0.6596\n",
      "Epoch 21/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.5314 - acc: 0.7478 - val_loss: 0.5009 - val_acc: 0.8085\n",
      "Epoch 22/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.5285 - acc: 0.7726 - val_loss: 0.4900 - val_acc: 0.7872\n",
      "Epoch 23/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4878 - acc: 0.7780 - val_loss: 0.4779 - val_acc: 0.7943\n",
      "Epoch 24/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4572 - acc: 0.7940 - val_loss: 0.4564 - val_acc: 0.8014\n",
      "Epoch 25/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4882 - acc: 0.7993 - val_loss: 0.4453 - val_acc: 0.8227\n",
      "Epoch 26/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.5030 - acc: 0.8224 - val_loss: 0.5360 - val_acc: 0.7376\n",
      "Epoch 27/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.5169 - acc: 0.7709 - val_loss: 0.5071 - val_acc: 0.7518\n",
      "Epoch 28/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.4670 - acc: 0.8011 - val_loss: 0.4464 - val_acc: 0.8298\n",
      "Epoch 29/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4890 - acc: 0.7993 - val_loss: 0.4751 - val_acc: 0.7730\n",
      "Epoch 30/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4249 - acc: 0.8153 - val_loss: 0.4760 - val_acc: 0.7730\n",
      "Epoch 31/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4496 - acc: 0.8011 - val_loss: 0.4438 - val_acc: 0.8085\n",
      "Epoch 32/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.5169 - acc: 0.8206 - val_loss: 0.4736 - val_acc: 0.7801\n",
      "Epoch 33/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.4271 - acc: 0.8277 - val_loss: 0.4642 - val_acc: 0.7943\n",
      "Epoch 34/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4843 - acc: 0.8348 - val_loss: 0.4407 - val_acc: 0.8156\n",
      "Epoch 35/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4697 - acc: 0.8259 - val_loss: 0.4457 - val_acc: 0.7943\n",
      "Epoch 36/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.4514 - acc: 0.8224 - val_loss: 0.4443 - val_acc: 0.8085\n",
      "Epoch 37/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.4512 - acc: 0.8206 - val_loss: 0.4431 - val_acc: 0.8085\n",
      "Epoch 38/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4241 - acc: 0.8171 - val_loss: 0.4390 - val_acc: 0.8085\n",
      "Epoch 39/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4669 - acc: 0.8206 - val_loss: 0.4348 - val_acc: 0.8227\n",
      "Epoch 40/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4340 - acc: 0.8348 - val_loss: 0.4382 - val_acc: 0.8014\n",
      "Epoch 41/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4209 - acc: 0.8437 - val_loss: 0.4344 - val_acc: 0.8298\n",
      "Epoch 42/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4564 - acc: 0.8135 - val_loss: 0.4458 - val_acc: 0.7872\n",
      "Epoch 43/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4539 - acc: 0.8117 - val_loss: 0.4796 - val_acc: 0.7660\n",
      "Epoch 44/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4389 - acc: 0.8259 - val_loss: 0.4376 - val_acc: 0.8298\n",
      "Epoch 45/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4032 - acc: 0.8348 - val_loss: 0.4397 - val_acc: 0.8014\n",
      "Epoch 46/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4355 - acc: 0.8206 - val_loss: 0.4400 - val_acc: 0.8014\n",
      "Epoch 47/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.5212 - acc: 0.8242 - val_loss: 0.4639 - val_acc: 0.7589\n",
      "Epoch 48/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.6621 - acc: 0.7229 - val_loss: 0.8150 - val_acc: 0.6738\n",
      "Epoch 49/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.6876 - acc: 0.7016 - val_loss: 0.7001 - val_acc: 0.6738\n",
      "Epoch 50/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.6105 - acc: 0.7087 - val_loss: 0.6047 - val_acc: 0.6950\n",
      "Epoch 51/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.5383 - acc: 0.7300 - val_loss: 0.5573 - val_acc: 0.7092\n",
      "Epoch 52/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4898 - acc: 0.7549 - val_loss: 0.5245 - val_acc: 0.7163\n",
      "Epoch 53/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4741 - acc: 0.7744 - val_loss: 0.5004 - val_acc: 0.7376\n",
      "Epoch 54/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4760 - acc: 0.7940 - val_loss: 0.4789 - val_acc: 0.7447\n",
      "Epoch 55/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4534 - acc: 0.7975 - val_loss: 0.4616 - val_acc: 0.7730\n",
      "Epoch 56/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4567 - acc: 0.8153 - val_loss: 0.4551 - val_acc: 0.7872\n",
      "Epoch 57/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4622 - acc: 0.8028 - val_loss: 0.4521 - val_acc: 0.7872\n",
      "Epoch 58/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4842 - acc: 0.8028 - val_loss: 0.4505 - val_acc: 0.7943\n",
      "Epoch 59/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4814 - acc: 0.7993 - val_loss: 0.4528 - val_acc: 0.8014\n",
      "Epoch 60/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4670 - acc: 0.7957 - val_loss: 0.4550 - val_acc: 0.8085\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 0s 37us/step - loss: 0.4936 - acc: 0.8188 - val_loss: 0.4576 - val_acc: 0.7943\n",
      "Epoch 62/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.4916 - acc: 0.8011 - val_loss: 0.4611 - val_acc: 0.7872\n",
      "Epoch 63/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.5117 - acc: 0.8117 - val_loss: 0.4613 - val_acc: 0.8085\n",
      "Epoch 64/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4349 - acc: 0.7869 - val_loss: 0.4726 - val_acc: 0.8085\n",
      "Epoch 65/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.4275 - acc: 0.7993 - val_loss: 0.5018 - val_acc: 0.7518\n",
      "Epoch 66/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4370 - acc: 0.8028 - val_loss: 0.5285 - val_acc: 0.7376\n",
      "Epoch 67/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4494 - acc: 0.8011 - val_loss: 0.4736 - val_acc: 0.7730\n",
      "Epoch 68/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4370 - acc: 0.8366 - val_loss: 0.4568 - val_acc: 0.8014\n",
      "Epoch 69/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.4357 - acc: 0.8401 - val_loss: 0.4555 - val_acc: 0.8014\n",
      "Epoch 70/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.4179 - acc: 0.8366 - val_loss: 0.4591 - val_acc: 0.8014\n",
      "Epoch 71/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4325 - acc: 0.8188 - val_loss: 0.4935 - val_acc: 0.7518\n",
      "Epoch 72/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4209 - acc: 0.8419 - val_loss: 0.4635 - val_acc: 0.7943\n",
      "Epoch 73/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.3944 - acc: 0.8366 - val_loss: 0.4576 - val_acc: 0.7872\n",
      "Epoch 74/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4336 - acc: 0.8277 - val_loss: 0.4548 - val_acc: 0.8014\n",
      "Epoch 75/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4208 - acc: 0.8295 - val_loss: 0.4547 - val_acc: 0.8227\n",
      "Epoch 76/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4107 - acc: 0.8295 - val_loss: 0.4582 - val_acc: 0.8014\n",
      "Epoch 77/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4241 - acc: 0.8437 - val_loss: 0.4616 - val_acc: 0.8156\n",
      "Epoch 78/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.4759 - acc: 0.8117 - val_loss: 0.5779 - val_acc: 0.7092\n",
      "Epoch 79/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4474 - acc: 0.7904 - val_loss: 0.5193 - val_acc: 0.7447\n",
      "Epoch 80/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4682 - acc: 0.8064 - val_loss: 0.4757 - val_acc: 0.7730\n",
      "Epoch 81/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4025 - acc: 0.8348 - val_loss: 0.4833 - val_acc: 0.7660\n",
      "Epoch 82/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4510 - acc: 0.8437 - val_loss: 0.4792 - val_acc: 0.7730\n",
      "Epoch 83/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.4371 - acc: 0.8188 - val_loss: 0.4703 - val_acc: 0.8085\n",
      "Epoch 84/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4131 - acc: 0.8259 - val_loss: 0.4631 - val_acc: 0.8085\n",
      "Epoch 85/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4349 - acc: 0.8330 - val_loss: 0.4581 - val_acc: 0.8156\n",
      "Epoch 86/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.3916 - acc: 0.8490 - val_loss: 0.4581 - val_acc: 0.8156\n",
      "Epoch 87/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.3958 - acc: 0.8437 - val_loss: 0.4594 - val_acc: 0.8156\n",
      "Epoch 88/200\n",
      "563/563 [==============================] - 0s 42us/step - loss: 0.3973 - acc: 0.8348 - val_loss: 0.4611 - val_acc: 0.8085\n",
      "Epoch 89/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.3926 - acc: 0.8401 - val_loss: 0.4623 - val_acc: 0.8156\n",
      "Epoch 90/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4226 - acc: 0.8401 - val_loss: 0.4628 - val_acc: 0.8085\n",
      "Epoch 91/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4264 - acc: 0.8437 - val_loss: 0.4643 - val_acc: 0.8085\n",
      "Epoch 92/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4194 - acc: 0.8330 - val_loss: 0.4774 - val_acc: 0.8156\n",
      "Epoch 93/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4288 - acc: 0.8437 - val_loss: 0.5696 - val_acc: 0.8156\n",
      "Epoch 94/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4037 - acc: 0.8259 - val_loss: 0.6396 - val_acc: 0.8156\n",
      "Epoch 95/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4470 - acc: 0.8171 - val_loss: 0.6391 - val_acc: 0.8227\n",
      "Epoch 96/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4227 - acc: 0.8348 - val_loss: 0.6431 - val_acc: 0.8085\n",
      "Epoch 97/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4375 - acc: 0.8295 - val_loss: 0.6416 - val_acc: 0.8085\n",
      "Epoch 98/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4422 - acc: 0.8117 - val_loss: 0.6445 - val_acc: 0.8085\n",
      "Epoch 99/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4326 - acc: 0.8188 - val_loss: 0.6502 - val_acc: 0.8014\n",
      "Epoch 100/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4474 - acc: 0.8135 - val_loss: 0.6467 - val_acc: 0.8085\n",
      "Epoch 101/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.4277 - acc: 0.8242 - val_loss: 0.6458 - val_acc: 0.8085\n",
      "Epoch 102/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3914 - acc: 0.8384 - val_loss: 0.6470 - val_acc: 0.8085\n",
      "Epoch 103/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4399 - acc: 0.8277 - val_loss: 0.6500 - val_acc: 0.7943\n",
      "Epoch 104/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4338 - acc: 0.8295 - val_loss: 0.6579 - val_acc: 0.7730\n",
      "Epoch 105/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3824 - acc: 0.8259 - val_loss: 0.6597 - val_acc: 0.7730\n",
      "Epoch 106/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.3826 - acc: 0.8384 - val_loss: 0.4935 - val_acc: 0.7730\n",
      "Epoch 107/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4124 - acc: 0.8366 - val_loss: 0.6506 - val_acc: 0.7943\n",
      "Epoch 108/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4485 - acc: 0.8313 - val_loss: 0.6456 - val_acc: 0.8156\n",
      "Epoch 109/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.3935 - acc: 0.8117 - val_loss: 0.6499 - val_acc: 0.8369\n",
      "Epoch 110/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4358 - acc: 0.8295 - val_loss: 0.6481 - val_acc: 0.8014\n",
      "Epoch 111/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.4058 - acc: 0.8259 - val_loss: 0.6615 - val_acc: 0.7801\n",
      "Epoch 112/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4086 - acc: 0.8171 - val_loss: 0.6585 - val_acc: 0.7801\n",
      "Epoch 113/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4226 - acc: 0.8313 - val_loss: 0.6470 - val_acc: 0.8085\n",
      "Epoch 114/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.3738 - acc: 0.8313 - val_loss: 0.6521 - val_acc: 0.7943\n",
      "Epoch 115/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4257 - acc: 0.8313 - val_loss: 0.6507 - val_acc: 0.7943\n",
      "Epoch 116/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.4474 - acc: 0.8295 - val_loss: 0.6481 - val_acc: 0.8085\n",
      "Epoch 117/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4281 - acc: 0.8259 - val_loss: 0.6533 - val_acc: 0.8085\n",
      "Epoch 118/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4195 - acc: 0.8011 - val_loss: 0.6523 - val_acc: 0.8014\n",
      "Epoch 119/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4194 - acc: 0.8259 - val_loss: 0.6522 - val_acc: 0.8014\n",
      "Epoch 120/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4211 - acc: 0.8242 - val_loss: 0.6532 - val_acc: 0.7943\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 0s 39us/step - loss: 0.3672 - acc: 0.8384 - val_loss: 0.6553 - val_acc: 0.7943\n",
      "Epoch 122/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3989 - acc: 0.8259 - val_loss: 0.6614 - val_acc: 0.7943\n",
      "Epoch 123/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.3740 - acc: 0.8277 - val_loss: 0.6613 - val_acc: 0.8014\n",
      "Epoch 124/200\n",
      "563/563 [==============================] - 0s 42us/step - loss: 0.3854 - acc: 0.8330 - val_loss: 0.6494 - val_acc: 0.7943\n",
      "Epoch 125/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4111 - acc: 0.8472 - val_loss: 0.6487 - val_acc: 0.8014\n",
      "Epoch 126/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4158 - acc: 0.8082 - val_loss: 0.6478 - val_acc: 0.7943\n",
      "Epoch 127/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4453 - acc: 0.8135 - val_loss: 0.6462 - val_acc: 0.7872\n",
      "Epoch 128/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4154 - acc: 0.8011 - val_loss: 0.6552 - val_acc: 0.7872\n",
      "Epoch 129/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4332 - acc: 0.8171 - val_loss: 0.7380 - val_acc: 0.7872\n",
      "Epoch 130/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.3633 - acc: 0.8117 - val_loss: 0.7407 - val_acc: 0.8085\n",
      "Epoch 131/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4445 - acc: 0.8082 - val_loss: 0.7001 - val_acc: 0.7589\n",
      "Epoch 132/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4330 - acc: 0.8135 - val_loss: 0.6148 - val_acc: 0.7518\n",
      "Epoch 133/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.3826 - acc: 0.8224 - val_loss: 0.6411 - val_acc: 0.7801\n",
      "Epoch 134/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4359 - acc: 0.7940 - val_loss: 0.6392 - val_acc: 0.8014\n",
      "Epoch 135/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4724 - acc: 0.8259 - val_loss: 0.6401 - val_acc: 0.7730\n",
      "Epoch 136/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3992 - acc: 0.8224 - val_loss: 0.6436 - val_acc: 0.7660\n",
      "Epoch 137/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4768 - acc: 0.8028 - val_loss: 0.6511 - val_acc: 0.7589\n",
      "Epoch 138/200\n",
      "563/563 [==============================] - 0s 38us/step - loss: 0.3779 - acc: 0.8011 - val_loss: 0.6611 - val_acc: 0.7518\n",
      "Epoch 139/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4218 - acc: 0.7886 - val_loss: 0.7427 - val_acc: 0.7518\n",
      "Epoch 140/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4262 - acc: 0.8135 - val_loss: 0.7529 - val_acc: 0.7518\n",
      "Epoch 141/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.4489 - acc: 0.8153 - val_loss: 0.7483 - val_acc: 0.7730\n",
      "Epoch 142/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4647 - acc: 0.7975 - val_loss: 0.7344 - val_acc: 0.7589\n",
      "Epoch 143/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.3936 - acc: 0.8064 - val_loss: 0.6632 - val_acc: 0.7660\n",
      "Epoch 144/200\n",
      "563/563 [==============================] - 0s 43us/step - loss: 0.4446 - acc: 0.7904 - val_loss: 0.7429 - val_acc: 0.7589\n",
      "Epoch 145/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4647 - acc: 0.7993 - val_loss: 0.7477 - val_acc: 0.7589\n",
      "Epoch 146/200\n",
      "563/563 [==============================] - 0s 42us/step - loss: 0.4328 - acc: 0.8046 - val_loss: 0.6781 - val_acc: 0.7801\n",
      "Epoch 147/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.4065 - acc: 0.7886 - val_loss: 0.7507 - val_acc: 0.7730\n",
      "Epoch 148/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4256 - acc: 0.7975 - val_loss: 0.7499 - val_acc: 0.7660\n",
      "Epoch 149/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4517 - acc: 0.7975 - val_loss: 0.8275 - val_acc: 0.7660\n",
      "Epoch 150/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4198 - acc: 0.8082 - val_loss: 0.7499 - val_acc: 0.7589\n",
      "Epoch 151/200\n",
      "563/563 [==============================] - 0s 42us/step - loss: 0.4292 - acc: 0.7993 - val_loss: 0.7504 - val_acc: 0.7589\n",
      "Epoch 152/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.3814 - acc: 0.8171 - val_loss: 0.6844 - val_acc: 0.7518\n",
      "Epoch 153/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4076 - acc: 0.8064 - val_loss: 0.7554 - val_acc: 0.7518\n",
      "Epoch 154/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4308 - acc: 0.7957 - val_loss: 0.7642 - val_acc: 0.7589\n",
      "Epoch 155/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.3952 - acc: 0.7993 - val_loss: 0.8204 - val_acc: 0.7447\n",
      "Epoch 156/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.4500 - acc: 0.7886 - val_loss: 0.7446 - val_acc: 0.6879\n",
      "Epoch 157/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.5284 - acc: 0.7478 - val_loss: 0.5541 - val_acc: 0.7021\n",
      "Epoch 158/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4517 - acc: 0.7869 - val_loss: 0.5113 - val_acc: 0.7447\n",
      "Epoch 159/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4435 - acc: 0.8082 - val_loss: 0.6625 - val_acc: 0.7801\n",
      "Epoch 160/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4334 - acc: 0.8153 - val_loss: 0.6449 - val_acc: 0.7376\n",
      "Epoch 161/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3666 - acc: 0.8206 - val_loss: 0.6475 - val_acc: 0.7376\n",
      "Epoch 162/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4270 - acc: 0.7922 - val_loss: 0.6435 - val_acc: 0.7376\n",
      "Epoch 163/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4441 - acc: 0.7922 - val_loss: 0.6460 - val_acc: 0.7589\n",
      "Epoch 164/200\n",
      "563/563 [==============================] - 0s 42us/step - loss: 0.4174 - acc: 0.7975 - val_loss: 0.6655 - val_acc: 0.7589\n",
      "Epoch 165/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.3957 - acc: 0.7922 - val_loss: 0.7354 - val_acc: 0.7589\n",
      "Epoch 166/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.3652 - acc: 0.8188 - val_loss: 0.6627 - val_acc: 0.7730\n",
      "Epoch 167/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4208 - acc: 0.7993 - val_loss: 0.8189 - val_acc: 0.7730\n",
      "Epoch 168/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4354 - acc: 0.7957 - val_loss: 0.8257 - val_acc: 0.7660\n",
      "Epoch 169/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3651 - acc: 0.7975 - val_loss: 0.8217 - val_acc: 0.7518\n",
      "Epoch 170/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4528 - acc: 0.7744 - val_loss: 0.8184 - val_acc: 0.7589\n",
      "Epoch 171/200\n",
      "563/563 [==============================] - 0s 34us/step - loss: 0.4114 - acc: 0.7833 - val_loss: 0.7430 - val_acc: 0.7447\n",
      "Epoch 172/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4117 - acc: 0.7957 - val_loss: 0.7318 - val_acc: 0.7589\n",
      "Epoch 173/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4196 - acc: 0.7851 - val_loss: 0.8201 - val_acc: 0.7660\n",
      "Epoch 174/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4264 - acc: 0.7957 - val_loss: 0.7464 - val_acc: 0.7589\n",
      "Epoch 175/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.4361 - acc: 0.8064 - val_loss: 0.7237 - val_acc: 0.7518\n",
      "Epoch 176/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3608 - acc: 0.8153 - val_loss: 0.6678 - val_acc: 0.7305\n",
      "Epoch 177/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4166 - acc: 0.8011 - val_loss: 0.6533 - val_acc: 0.7518\n",
      "Epoch 178/200\n",
      "563/563 [==============================] - 0s 35us/step - loss: 0.3852 - acc: 0.8028 - val_loss: 0.7283 - val_acc: 0.7660\n",
      "Epoch 179/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4156 - acc: 0.7780 - val_loss: 0.7344 - val_acc: 0.7660\n",
      "Epoch 180/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4309 - acc: 0.7869 - val_loss: 0.8239 - val_acc: 0.7589\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563/563 [==============================] - 0s 39us/step - loss: 0.4552 - acc: 0.7744 - val_loss: 0.7819 - val_acc: 0.7447\n",
      "Epoch 182/200\n",
      "563/563 [==============================] - 0s 32us/step - loss: 0.6403 - acc: 0.7762 - val_loss: 1.0434 - val_acc: 0.6879\n",
      "Epoch 183/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4637 - acc: 0.7833 - val_loss: 0.5051 - val_acc: 0.7518\n",
      "Epoch 184/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4099 - acc: 0.8259 - val_loss: 0.5702 - val_acc: 0.7872\n",
      "Epoch 185/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4064 - acc: 0.8188 - val_loss: 0.6445 - val_acc: 0.8014\n",
      "Epoch 186/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3910 - acc: 0.8224 - val_loss: 0.6411 - val_acc: 0.8085\n",
      "Epoch 187/200\n",
      "563/563 [==============================] - 0s 40us/step - loss: 0.4034 - acc: 0.8082 - val_loss: 0.6427 - val_acc: 0.8085\n",
      "Epoch 188/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.3796 - acc: 0.8295 - val_loss: 0.6432 - val_acc: 0.8085\n",
      "Epoch 189/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3674 - acc: 0.8171 - val_loss: 0.6429 - val_acc: 0.8085\n",
      "Epoch 190/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.3853 - acc: 0.8082 - val_loss: 0.6398 - val_acc: 0.8156\n",
      "Epoch 191/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3760 - acc: 0.8153 - val_loss: 0.6419 - val_acc: 0.8156\n",
      "Epoch 192/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4091 - acc: 0.8224 - val_loss: 0.6471 - val_acc: 0.8085\n",
      "Epoch 193/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4509 - acc: 0.8064 - val_loss: 0.7313 - val_acc: 0.8014\n",
      "Epoch 194/200\n",
      "563/563 [==============================] - 0s 41us/step - loss: 0.3508 - acc: 0.8259 - val_loss: 0.7588 - val_acc: 0.7943\n",
      "Epoch 195/200\n",
      "563/563 [==============================] - 0s 36us/step - loss: 0.3757 - acc: 0.8295 - val_loss: 0.7426 - val_acc: 0.8085\n",
      "Epoch 196/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.3771 - acc: 0.8028 - val_loss: 0.7254 - val_acc: 0.8014\n",
      "Epoch 197/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4337 - acc: 0.8011 - val_loss: 0.7412 - val_acc: 0.7943\n",
      "Epoch 198/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.4393 - acc: 0.7780 - val_loss: 0.7314 - val_acc: 0.7872\n",
      "Epoch 199/200\n",
      "563/563 [==============================] - 0s 37us/step - loss: 0.3673 - acc: 0.8242 - val_loss: 0.7376 - val_acc: 0.7660\n",
      "Epoch 200/200\n",
      "563/563 [==============================] - 0s 39us/step - loss: 0.4500 - acc: 0.7798 - val_loss: 0.8193 - val_acc: 0.7801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e2624ada0>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ManualModel.fit(np.array(trainX),np.array(trainY),epochs=200\n",
    "                , batch_size=50, validation_data=(valX, valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 420us/step\n"
     ]
    }
   ],
   "source": [
    "scores=firstModel.evaluate(np.array(valX),np.array(valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4691484462707601\n",
      "Accuracy 80.85106391433283\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\",scores[0])\n",
    "print(\"Accuracy\",scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondModel=getModel([66,100,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e24aaf438>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondModel.fit(np.array(trainX),np.array(trainY),epochs=40,callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "scores2=secondModel.evaluate(np.array(valX),np.array(valY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4524111253149966\n",
      "Accuracy 78.72340433986474\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\",scores2[0])\n",
    "print(\"Accuracy\",scores2[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters = {'solver': ['adam', 'sgd', 'lbfgs'], 'max_iter': [500,1000,1500], 'hidden_layer_sizes':[(7, 5, 4), (4,3), (20, 10, 5 , 4 , 3)], 'random_state' : [42]}\n",
    "# clf_grid = GridSearchCV(neural_network.MLPClassifier(), parameters, n_jobs=-1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_grid.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "# print(\"-----------------Original Features--------------------\")\n",
    "# print(\"Best score: %0.4f\" % clf_grid.best_score_)\n",
    "# print(\"Using the following parameters:\")\n",
    "# print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_best = clf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPClassifier(random_state=andom_state=0, solver='lbfgs', hidden_layer_sizes=[100])\n",
    "# mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.830897977285436"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(mlp_best, X_train, y = y_train, scoring = \"accuracy\", cv = 3, n_jobs=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881 classes\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_2_input to have shape (881,) but got array with shape (66,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-3dc0bc06f87a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sgd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_2_input to have shape (881,) but got array with shape (66,)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "dims = X_train.shape[0]\n",
    "nb_classes = y_train.shape[0]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nb_classes, input_shape=(dims,), activation='sigmoid'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state = 2\n",
    "# classifiers = []\n",
    "# classifiers.append(SVC(random_state=random_state))\n",
    "# classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "# classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "# classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "# classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "# classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "# classifiers.append(MLPClassifier(random_state=random_state))\n",
    "# classifiers.append(KNeighborsClassifier())\n",
    "# classifiers.append(LogisticRegression(random_state = random_state))\n",
    "# classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "# cv_results = []\n",
    "# for classifier in classifiers :\n",
    "#     cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "# cv_means = []\n",
    "# cv_std = []\n",
    "# for cv_result in cv_results:\n",
    "#     cv_means.append(cv_result.mean())\n",
    "#     cv_std.append(cv_result.std())\n",
    "\n",
    "# cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "# \"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "# g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "# g.set_xlabel(\"Mean Accuracy\")\n",
    "# g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaBoost, ExtraTrees , RandomForest, GradientBoosting u SVC classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DTC = DecisionTreeClassifier()\n",
    "\n",
    "# adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "# ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "#               \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "#               \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "#               \"n_estimators\" :[1,2],\n",
    "#               \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "# gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "# gsadaDTC.fit(X_train,Y_train)\n",
    "\n",
    "# ada_best = gsadaDTC.best_estimator_\n",
    "# gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtC = ExtraTreesClassifier()\n",
    "\n",
    "# ex_param_grid = {\"max_depth\": [None],\n",
    "#               \"max_features\": [1, 3, 10],\n",
    "#               \"min_samples_split\": [2, 3, 10],\n",
    "#               \"min_samples_leaf\": [1, 3, 10],\n",
    "#               \"bootstrap\": [False],\n",
    "#               \"n_estimators\" :[100,300],\n",
    "#               \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "# gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "# gsExtC.fit(X_train,Y_train)\n",
    "\n",
    "# ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC = RandomForestClassifier()\n",
    "\n",
    "# rf_param_grid = {\"max_depth\": [None],\n",
    "#               \"max_features\": [1, 3, 10],\n",
    "#               \"min_samples_split\": [2, 3, 10],\n",
    "#               \"min_samples_leaf\": [1, 3, 10],\n",
    "#               \"bootstrap\": [False],\n",
    "#               \"n_estimators\" :[100,300],\n",
    "#               \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "# gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "# gsRFC.fit(X_train,Y_train)\n",
    "\n",
    "# RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC = GradientBoostingClassifier()\n",
    "# gb_param_grid = {'loss' : [\"deviance\"],\n",
    "#               'n_estimators' : [100,200,300],\n",
    "#               'learning_rate': [0.1, 0.05, 0.01],\n",
    "#               'max_depth': [4, 8],\n",
    "#               'min_samples_leaf': [100,150],\n",
    "#               'max_features': [0.3, 0.1] \n",
    "#               }\n",
    "\n",
    "# gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "# gsGBC.fit(X_train,Y_train)\n",
    "\n",
    "# GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMC = SVC(probability=True)\n",
    "\n",
    "# svc_param_grid = {'kernel': ['rbf'], \n",
    "#                   'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "#                   'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "# gsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "# gsSVMC.fit(X_train,Y_train)\n",
    "\n",
    "# SVMC_best = gsSVMC.best_estimator_\n",
    "\n",
    "# gsSVMC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\n",
    "# test_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\n",
    "# test_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\n",
    "# test_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\n",
    "# test_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX = np.concatenate((RFC_best.predict(test),ExtC_best.predict(test), SVMC_best.predict(test),ada_best.predict(test),GBC_best.predict(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n",
    "# ('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "# votingC = votingC.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.median(cross_val_score(votingC, X_train, y = Y_train, scoring = \"accuracy\", cv = 5, n_jobs=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n",
    "\n",
    "# results = pd.concat([IDtest,test_Survived],axis=1)\n",
    "\n",
    "# results.to_csv(\"ddxk.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
