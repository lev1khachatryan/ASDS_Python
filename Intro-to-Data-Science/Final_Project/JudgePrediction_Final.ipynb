{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# from scipy.stats import randint as sp_randint\n",
    "# from sklearn.metrics import accuracy_score, r2_score\n",
    "# from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "# from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from mlxtend.regressor import StackingRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# import matplotlib.pyplot as plt\n",
    "# import xgboost\n",
    "# import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(conn, table, is_train = True, train_decision_date = None, test_id = None, load_from_disk = False):\n",
    "    if load_from_disk:\n",
    "        return pd.read_pickle(table + '.pkl')    \n",
    "    cursor = conn.cursor()       \n",
    "    sql = 'select * from dbo.load_' + table    \n",
    "    if table == 'CourtCase':\n",
    "        sql += '(?, ?)'\n",
    "        cursor.execute(sql, (test_id, train_decision_date))\n",
    "        rows = np.array(cursor.fetchall())\n",
    "        columns = np.array(cursor.description)[:, 0]            \n",
    "        assert len(rows) > 0, 'No data found for specified filters'\n",
    "    else:\n",
    "        sql += '(?)'\n",
    "        cursor.execute(sql, (test_id))\n",
    "        rows = np.array(cursor.fetchall())\n",
    "        columns = np.array(cursor.description)[:, 0]\n",
    "    data = pd.DataFrame(data=list(rows), columns=columns)\n",
    "    data.to_pickle(table + \".pkl\")\n",
    "    return data\n",
    "\n",
    "def load_all_data(conn, tables, is_train = True, train_decision_date = None, test_id = None, load_from_disk=False):\n",
    "    try:\n",
    "        assert is_train or test_id != None\n",
    "    except AssertionError():\n",
    "        raise AssertionError(\"test_id must be passed in training mode\") \n",
    "    results = {}    \n",
    "    for table in tables:\n",
    "        results[table] = load_data(conn, table, is_train, train_decision_date, test_id, load_from_disk=load_from_disk)\n",
    "        print(f'{table} data has been processed')            \n",
    "    return results\n",
    "\n",
    "def data_preprocessing(data, load_from_disk = False):\n",
    "    \n",
    "    CourtCase = data[\"CourtCase\"]\n",
    "    CourtCaseParty = data[\"CourtCaseParty\"]\n",
    "    CourtCaseSchedule = data[\"CourtCaseSchedule\"]\n",
    "    CourtCaseCrimes = data[\"CourtCaseCrimes\"]\n",
    "    CourtCaseDocument = data[\"CourtCaseDocument\"]\n",
    "    CourtCasePartyLegalRepresentative = data[\"CourtCasePartyLegalRepresentative\"]\n",
    "            \n",
    "    CourtCase = CourtCase.sample(frac=1)\n",
    "\n",
    "    CourtCaseParty[\"IsNPPA\"] = CourtCaseParty.PartyID.apply(lambda x: 1 if x == -2 else 0)\n",
    "\n",
    "    ccp_aggregations = {}\n",
    "    ccp_aggregations[\"CourtCasePartyID\"] = {\"ccp_count\": \"count\"}\n",
    "    ccp_aggregations[\"IsNPPA\"] = {\"is_nppa_present\": \"max\"}\n",
    "\n",
    "    CourtCaseParty = CourtCaseParty.groupby(\"CourtCaseID\").agg({**ccp_aggregations})\n",
    "    CourtCaseParty.columns = CourtCaseParty.columns.droplevel(level=0)\n",
    "    CourtCase = pd.merge(CourtCase, CourtCaseParty, how='left', left_on=\"CourtCaseID\", right_index=True)\n",
    "    \n",
    "\n",
    "    ccsh_aggregations = {\"HearingDate\" : {\"min_hearingdate\": \"min\"}}\n",
    "    CourtCaseSchedule = CourtCaseSchedule.groupby(\"CourtCaseID\").agg({**ccsh_aggregations})\n",
    "    CourtCaseSchedule.columns = CourtCaseSchedule.columns.droplevel(level=0)\n",
    "    CourtCase = pd.merge(CourtCase, CourtCaseSchedule, how='left', left_on=\"CourtCaseID\", right_index=True)\n",
    "\n",
    "\n",
    "    CourtCaseCrimes = CourtCaseCrimes.groupby(\"CourtCaseID\").count()\n",
    "    CourtCase = pd.merge(CourtCase, CourtCaseCrimes, how='left', left_on=\"CourtCaseID\", right_index=True)\n",
    "\n",
    "    CourtCase = pd.merge(CourtCase, CourtCasePartyLegalRepresentative, how='left', left_on=\"CourtCaseID\", right_on=\"CourtCaseID\")\n",
    "\n",
    "\n",
    "    # CourtCaseAddmisibility = pd.read_csv(\"CourtCaseAddmisibility.csv\")\n",
    "    # ccai_aggregations = {\"AdmissibilityItemID\" : {\"cnt_admitem\" : \"count\"}}\n",
    "    # CourtCaseAddmisibility = CourtCaseAddmisibility.groupby(\"CourtCaseID\").agg({**ccai_aggregations})\n",
    "    # CourtCaseAddmisibility.columns = CourtCaseAddmisibility.columns.droplevel(level=0)\n",
    "    # CourtCase = pd.merge(CourtCase, CourtCaseAddmisibility, how='left', left_on=\"CourtCaseID\", right_index=True)    \n",
    "\n",
    "    # CourtCaseIssues = pd.read_csv(\"CourtCaseIssues.csv\")\n",
    "    # ccissues_aggregations = {\"CourtCaseIssuesToBeAnalysedID\" : {\"cnt_issues\" : \"count\"}}\n",
    "    # CourtCaseIssues = CourtCaseIssues.groupby(\"CourtCaseID\").agg({**ccissues_aggregations})\n",
    "    # CourtCaseIssues.columns = CourtCaseIssues.columns.droplevel(level=0)\n",
    "    # CourtCase = pd.merge(CourtCase, CourtCaseIssues, how='left', left_on=\"CourtCaseID\", right_index=True)    \n",
    "\n",
    "    CourtCaseDocument = pd.get_dummies(CourtCaseDocument, columns=[\"DocumentTypeID\"])\n",
    "\n",
    "    cc_doc_dum_agg = {}\n",
    "    dum_columns = [x for x in CourtCaseDocument.columns if x.startswith(\"DocumentTypeID\")]\n",
    "    for col in dum_columns:\n",
    "        cc_doc_dum_agg[col] = {col:\"sum\"}\n",
    "    ccdoc_aggregations = {\"Size\" : {\"total_size\" : \"sum\", \"avg_size\" : \"mean\"}}\n",
    "    CourtCaseDocument.Size = pd.to_numeric(CourtCaseDocument.Size)    \n",
    "\n",
    "    CourtCaseDocument = CourtCaseDocument.groupby(\"CourtCaseID\").agg({**ccdoc_aggregations, **cc_doc_dum_agg})\n",
    "    CourtCaseDocument.columns = CourtCaseDocument.columns.droplevel(level=0)\n",
    "    CourtCase = pd.merge(CourtCase, CourtCaseDocument, how='left', left_on=\"CourtCaseID\", right_index=True)\n",
    "\n",
    "    [CourtCase[x].fillna(0, inplace=True) for x in CourtCase.columns if x.startswith(\"ArticleID\")];\n",
    "    CourtCase.CountOfLegalRepresentative.fillna(0, inplace=True)\n",
    "    CourtCase.ccp_count.fillna(0, inplace=True)\n",
    "    # CourtCase.cnt_admitem.fillna(0, inplace=True)\n",
    "    # CourtCase.cnt_issues.fillna(0, inplace=True)\n",
    "    CourtCase.is_nppa_present.fillna(0, inplace=True)        \n",
    "\n",
    "    CourtCase[\"HasRecieptDocument\"] = CourtCase.ReceiptDocumentID.fillna(0).apply(lambda x: 1 if x > 0 else 0)\n",
    "    CourtCase[\"HasProsecutionCase\"] = CourtCase.ProsecutionCaseID.fillna(0).apply(lambda x: 1 if x > 0 else 0)\n",
    "    # CourtCase[\"IsAppealedcase\"] = CourtCase.AppealedCourtCaseID.fillna(0).apply(lambda x: 1 if x > 0 else 0)\n",
    "    CourtCase[\"ColorID\"] = CourtCase.ColorID.fillna(-1)\n",
    "    CourtCase[\"InstanceLevelID\"] = CourtCase.InstanceLevelID.fillna(-1)\n",
    "    CourtCase[\"SubCategoryID\"] = CourtCase.SubCategoryID.fillna(-1)\n",
    "    CourtCase[\"CasePriorityID\"] = CourtCase.CasePriorityID.fillna(-1)\n",
    "    CourtCase[\"IsDetentionCase\"] = CourtCase.IsDetentionCase.fillna(0)\n",
    "    CourtCase[\"IsPublicCase\"] = CourtCase.IsPublicCase.fillna(0)\n",
    "    CourtCase[\"CommittedByMinor\"] = CourtCase.CommittedByMinor.fillna(0)\n",
    "    CourtCase[\"GenderBasedViolence\"] = CourtCase.GenderBasedViolence.fillna(0)\n",
    "    CourtCase[\"InitiatedFromAbunzi\"] = CourtCase.InitiatedFromAbunzi.fillna(0)\n",
    "    CourtCase[\"SolvedFromAbunzi\"] = CourtCase.SolvedFromAbunzi.fillna(0)\n",
    "    CourtCase[\"HasDetails\"] = CourtCase.HasDetails.fillna(0)\n",
    "    CourtCase[\"IsExempted\"] = CourtCase.IsExempted.fillna(0)\n",
    "    CourtCase[\"AttachedDate\"] = CourtCase.AttachedDate.fillna(0)\n",
    "    CourtCase.drop(columns=[\"HasPassedCaseNumberAllocated\", \"CaseCode\", \"MinorVersion\", \"MajorVersion\", \"CourtID\", \"CourtCaseID\"\n",
    "                       , \"ReceiptDocumentID\",  \"ProsecutionCaseID\", \"WFActionID\"\n",
    "                       , \"NotRegisteredCaseCode\", \"WFStateID\", \"UpdatedUserID\", \"OwnerUserID\", \"PublicOwnerUserId\", 'CreatedUserID'\n",
    "                       ,'AppealedCourtCaseID', \"CountOfJudgmentPages\"\n",
    "                      ], inplace=True)    \n",
    "  \n",
    "    drop_col = ['DecisionPronouncementDate', 'DecisionPronouncementDateYearID',\n",
    "       'ExecutionCaseApprovedUserID', 'PaymentBankID', 'LitigationCaseID',\n",
    "       'CaseRejectionID', 'ExtraOrdinaryProcedureID', 'PreviousCourtCaseID',\n",
    "       'SpecialCaseID']\n",
    "    CourtCase = CourtCase.drop(columns=drop_col)\n",
    "    CourtCase.dropna(inplace=True)\n",
    "    CourtCase.FillingFee = pd.to_numeric(CourtCase.FillingFee)\n",
    "    \n",
    "    assert len(CourtCase) > 0, 'No data left after preprocessing'    \n",
    "\n",
    "    X = CourtCase.drop(columns=[\"DecisionDuration\"])\n",
    "    Y = CourtCase[\"DecisionDuration\"]\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection(path):\n",
    "    cwd = os.getcwd()\n",
    "    with open(cwd + path) as f:\n",
    "        data = json.load(f)\n",
    "    conn = pyodbc.connect(\"DRIVER={{SQL Server}};SERVER={0}; database={1}; \\\n",
    "           trusted_connection=no;UID={2};PWD={3}\".format(data[\"server\"]\n",
    "                                                         , data[\"db\"]\n",
    "                                                         , data[\"login\"]\n",
    "                                                         , data[\"pass\"]))\n",
    "    return conn\n",
    "\n",
    "def parse_args_train(*argument_array):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--properties_path',\n",
    "                        default = '\\db_properties\\db_connection.json',\n",
    "                        help='relative path to database connection properties')\n",
    "    parser.add_argument('--train_decision_date',                        \n",
    "                        help='date until where to train the data')\n",
    "    args = parser.parse_args(*argument_array)\n",
    "    return args\n",
    "\n",
    "def parse_args_test(*argument_array):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test_id',                        \n",
    "                        help='case id to test the duration', )    \n",
    "    args = parser.parse_args(*argument_array)\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args, load_from_disk=False):\n",
    "    TABLE_LIST = [\"CourtCase\", \"CourtCaseParty\", \"CourtCaseSchedule\"\n",
    "    , \"CourtCaseCrimes\", \"CourtCasePartyLegalRepresentative\", \"CourtCaseDocument\"]\n",
    "    PROPERTIES_PATH = '\\db_properties\\db_connection.json'\n",
    "    TRAIN_DECISION_DATE = '2018-05-16'\n",
    "\n",
    "#     boost_params = {'n_estimators': 200,\n",
    "#  'min_samples_split': 40,\n",
    "#  'min_samples_leaf': 4,\n",
    "#  'max_features': 'sqrt',\n",
    "#  'max_depth': 20,\n",
    "#  'learning_rate': 0.05}\n",
    "#     boost = GradientBoostingRegressor(**boost_params)\n",
    "\n",
    "    train_data = load_all_data(get_connection(PROPERTIES_PATH), TABLE_LIST, is_train=True\n",
    "                               , train_decision_date=TRAIN_DECISION_DATE, load_from_disk = True)\n",
    "    train_data = data_preprocessing(train_data, load_from_disk=True)\n",
    "    train_X, train_Y = train_data\n",
    "#     boost.fit(train_X, train_Y)\n",
    "#     return(train_X)\n",
    "    xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "    \n",
    "    print(cross_val_score(xgb, train_X, train_Y, cv=5, verbose=True))\n",
    "\n",
    "    print(\"training has been completed succesfully !!!!\")\n",
    "    print(\"--------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--test_id TEST_ID]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-561136a1-a5de-4da1-9903-22138e578749.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36-64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    args = parse_args_test()\n",
    "    TEST_ID = args.test_id\n",
    "\n",
    "    test_data = load_all_data(get_connection(PROPERTIES_PATH), TABLE_LIST, is_train=False, train_decision_date=None, test_id=TEST_ID)\n",
    "    test_data = data_preprocessing(test_data)\n",
    "    test_X, test_Y = test_data\n",
    "\n",
    "    missing_cols = set(train_X.columns ) - set(test_X.columns )\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        test_X[c] = 0\n",
    "    # Ensure the order of column in the test set is in the same order than in train set\n",
    "    test_X = test_X[train_X.columns]\n",
    "    y_pred = boost.predict(test_X)[0]\n",
    "\n",
    "    print(\"predicted duration is %f days\" % y_pred)\n",
    "    print(\"actual duration is %f days\" % test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CourtCase data has been processed\n",
      "CourtCaseParty data has been processed\n",
      "CourtCaseSchedule data has been processed\n",
      "CourtCaseCrimes data has been processed\n",
      "CourtCasePartyLegalRepresentative data has been processed\n",
      "CourtCaseDocument data has been processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36-64\\lib\\site-packages\\pandas\\core\\groupby.py:4291: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65129138 0.62876545 0.66043191 0.62452456 0.70251315]\n",
      "training has been completed succesfully !!!!\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "main(None, load_from_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.columns[A.dtypes == 'O']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
